# -*- coding: utf-8 -*-
"""NLP_Submission_Vol_1 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZLtEorTES3CrQZH4NE0Mb_fOVQmkZ-cn
"""

import pandas as pd
import re

data1 = pd.read_csv('Tweets.csv')

data1

data_baru = data1.drop(columns=['tweet_id','airline_sentiment_confidence','negativereason','negativereason_confidence',
                                'airline','airline_sentiment_gold','name','negativereason_gold','retweet_count',
                                'tweet_coord','tweet_created','tweet_location','user_timezone'])

data_baru.isnull().sum()

gabung = data_baru.append(data_baru, ignore_index=True)

def hapus_simbol(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)
        
    return input_txt

gabung['text_baru'] = np.vectorize(hapus_simbol)(gabung['text'], "@[\w]*")

gabung['text_baru'] = gabung['text_baru'].str.replace("[^a-zA-Z#]", " ")

gabung['text_baru'] = gabung['text_baru'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))

new_text = gabung.drop(columns='text')
new_text.head()

sentiment = pd.get_dummies(new_text.airline_sentiment)
df_baru = pd.concat([new_text, sentiment], axis=1)
df_baru = df_baru.drop(columns='airline_sentiment')
df_baru

comment = df_baru['text_baru'].values
label = df_baru[['negative', 'neutral', 'positive']].values

print(df_baru)

from sklearn.model_selection import train_test_split
data_latih, data_test, label_latih, label_test = train_test_split(comment, label, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=10000, oov_token='flight')
tokenizer.fit_on_texts(data_latih)
 
sekuens_latih = tokenizer.texts_to_sequences(data_latih)
sekuens_test = tokenizer.texts_to_sequences(data_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.92 and logs.get('val_accuracy') > 0.92) :
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

num_epochs = 20
history = model.fit(padded_latih, label_latih, epochs=num_epochs, batch_size=128, callbacks=[callbacks],
                    validation_data=(padded_test, label_test), verbose=2)

import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

plt.plot(history.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()

plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Validasi'], loc='upper right')
plt.show()

plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Validasi'], loc='lower right')
plt.show()